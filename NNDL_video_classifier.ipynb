{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNDL_video_classifier.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/passerbyWt/videoClassification/blob/main/NNDL_video_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyukY2tQoWqr"
      },
      "source": [
        "# define constants\n",
        "DATA_DIR = \"content/UCF101\"\n",
        "LABEL_DIR = \"content/UCF101_labels\"\n",
        "CACHE_DIR = \"content/cache\"\n",
        "FRAMES_PER_CLIP = 10\n",
        "IMG_SIZE = 224    # video frames would be resized to IMG_SIZE * IMG_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd7WITAffqnu"
      },
      "source": [
        "# Download UCF101 dataset and extract frames\n",
        "References\n",
        "\n",
        "\n",
        "*   https://www.kaggle.com/pevogam/starter-ucf101-with-pytorch\n",
        "*   https://blog.csdn.net/HW140701/article/details/115864277\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyH_CEpwfzM3"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from prettytable import PrettyTable\n",
        "from multiprocessing import Pool\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# !pip install av\n",
        "# import av\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "OS = sys.platform\n",
        "\n",
        "if not OS == 'win32' and not os.path.exists(CACHE_DIR):\n",
        "  !mkdir -p $CACHE_DIR\n",
        "if not OS == 'win32' and not os.path.exists(DATA_DIR):\n",
        "  !mkdir -p $DATA_DIR\n",
        "  !wget --no-check-certificate --limit-rate 100M -O UCF101.rar https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n",
        "  !unrar x ./UCF101.rar $DATA_DIR > /dev/null\n",
        "if not OS == 'win32' and not os.path.exists(LABEL_DIR):\n",
        "  !mkdir $LABEL_DIR\n",
        "  !wget --no-check-certificate -O UCF101_labels.zip https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip\n",
        "  !unzip -d $LABEL_DIR ./UCF101_labels.zip > /dev/null\n",
        "\n",
        "# check if data is ready\n",
        "if (os.path.exists(DATA_DIR+'/UCF-101/ApplyEyeMakeup') and os.path.exists(LABEL_DIR+'/ucfTrainTestlist')):\n",
        "  print(\"ready to go\")\n",
        "else:\n",
        "  print(\"Failed to download data\\nPlease manually download files from\\nhttps://www.crcv.ucf.edu/data/UCF101/UCF101.rar\\nand\\nhttps://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbvCGdDYmrmB"
      },
      "source": [
        "# enable GPU\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Using GPU!\")\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  print(\"Using CPU... this is going to be slow...\")\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKrw7uEIlrNn"
      },
      "source": [
        "# video loader\n",
        "'''\n",
        "  params:\n",
        "    1. the path the video (e.g. \"ApplyEyeMakepu/v_ApplyEyeMakeup_g01_c01.avi\")\n",
        "  return:\n",
        "    sample FRAMES_PER_CLIP frames from the video (evenly distributed along the timeline),\n",
        "    return a tensor (FRAMES_PER_CLIP x height x width x color_channels) that stores these frames\n",
        "'''\n",
        "def video_loader(filename):\n",
        "  filename = DATA_DIR + '/UCF-101/' + filename\n",
        "  if not os.path.exists(filename):\n",
        "    raise Exception(\"Cannot find file \" + filename)\n",
        "  frames = []\n",
        "  cap = cv2.VideoCapture(filename)\n",
        "  while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    else:\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "      frames.append(frame)\n",
        "  cap.release()\n",
        "  if len(frames) < 1:\n",
        "    raise Exception(\"Invalid video\")\n",
        "  if len(frames) > FRAMES_PER_CLIP:\n",
        "    ratio = len(frames) / FRAMES_PER_CLIP\n",
        "    frames_ = []\n",
        "    for i in range(FRAMES_PER_CLIP):\n",
        "      idx = int(i * ratio)\n",
        "      frames_.append(frames[idx])\n",
        "    frames = frames_\n",
        "  frames = np.stack(frames, axis=0)\n",
        "  return torch.tensor(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSDWKkz4FY57"
      },
      "source": [
        "v_test = video_loader('ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi')\n",
        "v_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwNWQ7Cw2P-N"
      },
      "source": [
        "# class mapping\n",
        "class_to_id = {}\n",
        "id_to_class = {}\n",
        "with open(LABEL_DIR+'/ucfTrainTestlist/classInd.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.split()\n",
        "    line[0] = int(line[0]) - 1\n",
        "    class_to_id[line[1]] = line[0]\n",
        "    id_to_class[line[0]] = line[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji4iWH9S6xPW"
      },
      "source": [
        "# define the UCF101 dataset class\n",
        "class UCF101(Dataset):\n",
        "  def __init__(self, _class_to_id, _subset, _video_loader, _transform=None):\n",
        "    if _subset == 'train':\n",
        "      train_data = []\n",
        "      with open(LABEL_DIR+'/ucfTrainTestlist/trainlist01.txt', 'r') as f1:\n",
        "        for i, line in enumerate(f1):\n",
        "          if i % 5 == 0:\n",
        "            # save that for dev set\n",
        "            continue\n",
        "          line = line.split()\n",
        "          train_data.append((int(line[1])-1, line[0]))   # (caption, video_filename)\n",
        "      self.data = train_data\n",
        "      f1.close()\n",
        "    elif _subset == 'test':\n",
        "      test_data = []\n",
        "      with open(LABEL_DIR+'/ucfTrainTestlist/testlist01.txt', 'r') as f1:\n",
        "        for line in f1:\n",
        "          line_ = line.split('/')\n",
        "          test_data.append((_class_to_id[line_[0]], line.strip()))\n",
        "      self.data = test_data\n",
        "      f1.close()\n",
        "    elif _subset == 'dev':\n",
        "      dev_data = []\n",
        "      with open(LABEL_DIR+'/ucfTrainTestlist/trainlist01.txt', 'r') as f1:\n",
        "        for i, line in enumerate(f1):\n",
        "          if i % 5 != 0:\n",
        "            # the sample is already in training set\n",
        "            continue\n",
        "          line = line.split()\n",
        "          dev_data.append((int(line[1])-1, line[0]))\n",
        "      self.data = dev_data\n",
        "      f1.close()\n",
        "    else:\n",
        "      raise Exception(\"_subset should have value 'train', 'test', or 'dev'\")\n",
        "    self.video_loader = _video_loader\n",
        "    self.transform = _transform\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    res = self.data[idx]\n",
        "    enc_video = self.video_loader(res[1])\n",
        "    if self.transform is not None:\n",
        "      enc_video = self.transform(enc_video)\n",
        "    return (res[0], enc_video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj5pJb9fqQU_"
      },
      "source": [
        "def custom_collate(batch):\n",
        "  captions, frames = [], []\n",
        "  for caption, frame in batch:\n",
        "    captions.append(caption)    # label of current video sample\n",
        "    frames.append(frame)        # sampled sequence of frames from the video\n",
        "  return (\n",
        "    torch.tensor(captions),\n",
        "    pad_sequence(frames, batch_first=True)\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_OmIBbKPmlH"
      },
      "source": [
        "def tfs(enc_video):\n",
        "  enc_video = torch.permute(enc_video, [0, 3, 1, 2]).float() / 255\n",
        "  transfrom = torch.nn.Sequential(\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "  )\n",
        "  return enc_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV-Ugtohr2Dq"
      },
      "source": [
        "# Load pre-trained image classifier\n",
        "Model:\n",
        "*   CoAtNet (Zihang Dai, et al. 2021)\n",
        "\n",
        "References:\n",
        "*   <a href=\"https://arxiv.org/abs/2106.04803\">Research Paper</a>\n",
        "*   <a href=\"https://github.com/chinhsuanwu/coatnet-pytorch/blob/master/coatnet.py\">Code</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7JsThYtsqHI"
      },
      "source": [
        "# download model\n",
        "if not OS == 'win32' and not os.path.exists('model'):\n",
        "  !mkdir -p model/coatnet\n",
        "  !wget -O model/coatnet/. --no-check-certificate https://github.com/chinhsuanwu/coatnet-pytorch/raw/master/coatnet.py\n",
        "from model.coatnet.coatnet import CoAtNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As-4cDjypsEv"
      },
      "source": [
        "**Define the image classification model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "  if feature_extracting:\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "def ResNet(num_classes, use_pretrained=False):\n",
        "  model = models.resnet152(pretrained=use_pretrained)\n",
        "  set_parameter_requires_grad(model, use_pretrained)\n",
        "  num_ftrs = model.fc.in_features\n",
        "  model.fc = torch.nn.Sequential(\n",
        "      torch.nn.Linear(num_ftrs, num_classes),\n",
        "      torch.nn.Sigmoid()\n",
        "  )\n",
        "  return model.to(device)"
      ],
      "metadata": {
        "id": "4eD3_BxcpM_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmDBDQxCo_te"
      },
      "source": [
        "# create an instance of the image classification model\n",
        "# image_classifier = CoAtNet((IMG_SIZE, IMG_SIZE), 3, [2,2,3,5,2], [64,96,192,384,768], num_classes=len(id_to_class.keys())).to(device)\n",
        "image_classifier = ResNet(len(id_to_class.keys()), use_pretrained=True)\n",
        "\n",
        "# visualize the model\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "# count_parameters(image_classifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-j2nlX5py_l"
      },
      "source": [
        "**Train the image classification model with UCF101 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOpi9NSrUDQG"
      },
      "source": [
        "batch_size = 2\n",
        "train_loader = DataLoader(\n",
        "    UCF101(class_to_id, 'train', video_loader, _transform=tfs),\n",
        "    collate_fn=custom_collate,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "dev_loader = DataLoader(\n",
        "    UCF101(class_to_id, 'dev', video_loader, _transform=tfs),\n",
        "    collate_fn=custom_collate,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "optimizer = Adam(image_classifier.parameters(), lr=0.001)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "def handle_batch(model, criterion, batch, _device):\n",
        "  captions, clips = batch\n",
        "  # move tensors to device\n",
        "  captions = captions.to(_device)\n",
        "  clips = clips.to(_device)\n",
        "  # extract frames\n",
        "  captions = captions.repeat_interleave(clips.shape[1])\n",
        "  clips = clips.view(-1, 3, IMG_SIZE, IMG_SIZE)\n",
        "  preds = model(clips)\n",
        "  loss = criterion(preds, captions)\n",
        "  return preds, loss\n",
        "\n",
        "# training params\n",
        "max_epoch = 10\n",
        "early_stopping = -0.000001\n",
        "show_train_loss = False   # disable this could speedup training\n",
        "\n",
        "# try loading trained model\n",
        "# if os.path.exists(CACHE_DIR+'/image_classifier.pt'):\n",
        "#   image_classifier.load_state_dict(torch.load(CACHE_DIR+'/image_classifier.pt'))\n",
        "#   print(\"model state loaded\")\n",
        "\n",
        "best_loss = None\n",
        "for epoch in range(max_epoch):\n",
        "  tot_train_loss = 0.0\n",
        "  tot_eval_loss = 0.0\n",
        "  image_classifier.train()\n",
        "  with tqdm(total=len(train_loader)+len(dev_loader)) as pbar:\n",
        "    for batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      _, loss = handle_batch(image_classifier, criterion, batch, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if show_train_loss:\n",
        "        tot_train_loss += loss.item()\n",
        "      pbar.update(1)\n",
        "    image_classifier.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in dev_loader:\n",
        "        _, loss = handle_batch(image_classifier, criterion, batch, device)\n",
        "        tot_eval_loss += loss.item()\n",
        "        pbar.update(1)\n",
        "  \n",
        "  if show_train_loss:\n",
        "    avg_train_loss = tot_train_loss / (len(train_loader) * batch_size * FRAMES_PER_CLIP)\n",
        "    avg_eval_loss = tot_eval_loss / (len(dev_loader) * batch_size * FRAMES_PER_CLIP)\n",
        "    tqdm.write(\"epoch={} avg_train_loss={:.4f} avg_eval_loss={:.4f}\".format(epoch, avg_train_loss, avg_eval_loss))\n",
        "  else:\n",
        "    avg_eval_loss = tot_eval_loss / (len(dev_loader) * batch_size * FRAMES_PER_CLIP)\n",
        "    tqdm.write(\"epoch={} avg_eval_loss={:.4f}\".format(epoch, avg_eval_loss))\n",
        "\n",
        "  # stopping criteria\n",
        "  if best_loss is None:\n",
        "    best_loss = tot_eval_loss\n",
        "  if tot_eval_loss > (best_loss * (1+early_stopping)):\n",
        "    tqdm.write(\"Eval loss not improving, stop training.\")\n",
        "    break\n",
        "  elif tot_eval_loss < best_loss:\n",
        "    best_loss = tot_eval_loss\n",
        "    # save weights periodically\n",
        "    torch.save(image_classifier.state_dict(), CACHE_DIR+'/image_classifier.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the trained image classifier**"
      ],
      "metadata": {
        "id": "6jyFcTF5-oPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_classifier = CoAtNet((IMG_SIZE, IMG_SIZE), 3, [2,2,3,5,2], [64,96,192,384,768], num_classes=len(id_to_class.keys())).to(device)\n",
        "# image_classifier = ResNet(len(id_to_class.keys()))\n",
        "\n",
        "try:\n",
        "  image_classifier.load_state_dict(torch.load(CACHE_DIR+'/image_classifier.pt'))\n",
        "except Exception as e:\n",
        "  print(\"Exception: \" + str(e))\n",
        "  print(\"Please make sure the 'image_classifier.pt' file exists\")"
      ],
      "metadata": {
        "id": "Iruj6hM--j5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "test_loader = DataLoader(\n",
        "    UCF101(class_to_id, 'test', video_loader, _transform=tfs),\n",
        "    collate_fn=custom_collate,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "def handle_batch(model, criterion, batch, _device):\n",
        "  captions, clips = batch\n",
        "  # move tensors to device\n",
        "  captions = captions.to(_device)\n",
        "  clips = clips.to(_device)\n",
        "  # extract frames\n",
        "  captions = captions.repeat_interleave(clips.shape[1])\n",
        "  clips = clips.view(-1, 3, IMG_SIZE, IMG_SIZE)\n",
        "  preds = model(clips)\n",
        "  loss = criterion(preds, captions)\n",
        "  preds_ = torch.argmax(preds, dim=1)\n",
        "  acc = (preds_ == captions)\n",
        "  return acc, loss\n",
        "\n",
        "image_classifier.eval()\n",
        "tot_eval_loss = 0.0\n",
        "with torch.no_grad():\n",
        "  accs = []\n",
        "  for batch in tqdm(test_loader):\n",
        "    acc, loss = handle_batch(image_classifier, criterion, batch, device)\n",
        "    tot_eval_loss += loss.cpu().item()\n",
        "    accs.append(acc)\n",
        "  accs = torch.cat(accs, dim=0)\n",
        "  num_of_samples = accs.shape[0]\n",
        "  avg_eval_loss = tot_eval_loss / (len(test_loader) * batch_size * FRAMES_PER_CLIP)\n",
        "  tqdm.write(\"accuracy={:.4f}, avg loss={:.4f}\".format(accs.sum().item() / num_of_samples, avg_eval_loss))"
      ],
      "metadata": {
        "id": "tqQ746kPBJqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "testset = UCF101(class_to_id, 'test', video_loader, _transform=tfs)"
      ],
      "metadata": {
        "id": "o6FlVSChrOYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testset = UCF101(class_to_id, 'test', video_loader, _transform=tfs)\n",
        "sample1 = testset[0]"
      ],
      "metadata": {
        "id": "dGWHFuBlLtpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample1[1][0,:,:,:]"
      ],
      "metadata": {
        "id": "2H4rE9QwLyco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "test_loader = DataLoader(\n",
        "    testset,\n",
        "    collate_fn=custom_collate,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "label_0, img_0 = next(iter(test_loader))\n",
        "label_0 = label_0.repeat_interleave(img_0.shape[1])\n",
        "img_0 = img_0.view(-1, 3, IMG_SIZE, IMG_SIZE)"
      ],
      "metadata": {
        "id": "jlC_k9QcOVPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 5\n",
        "print(id_to_class[label_0[idx].item()])\n",
        "plt.imshow(img_0[idx,:,:,:].permute(1,2,0))"
      ],
      "metadata": {
        "id": "dyZKh4NWPRfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 826\n",
        "label_1, img_1 = testset[idx]\n",
        "print(id_to_class[label_1])\n",
        "print(img_1.shape)\n",
        "plt.imshow(img_1[0,:,:,:].permute(1,2,0))"
      ],
      "metadata": {
        "id": "qB275oaaTNzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 7\n",
        "pred_ = image_classifier(img_0.view(-1,3,224,224).to(device))\n",
        "prob, pred = torch.max(pred_, dim=1)\n",
        "#print(pred.shape)\n",
        "print(\"Predicted class is '{}' with {:.4f} confidence\".format(id_to_class[pred[idx].item()], prob[idx].item()))\n",
        "plt.imshow(img_0[idx,:,:,:].permute(1,2,0))"
      ],
      "metadata": {
        "id": "eMj1s12YvzN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM video classifier"
      ],
      "metadata": {
        "id": "sVYSWZumqeqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "aBjmUFCRqpwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_use = 'ResNet'\n",
        "use_pretrained = True\n",
        "\n",
        "if model_to_use == 'CoAtNet':\n",
        "  image_classifier = CoAtNet((IMG_SIZE, IMG_SIZE), 3, [2,2,3,5,2], [64,96,192,384,768], num_classes=len(id_to_class.keys())).to(device)\n",
        "else:\n",
        "  image_classifier = ResNet(len(id_to_class.keys()))\n",
        "\n",
        "if use_pretrained:\n",
        "  try:\n",
        "    # image_classifier.load_state_dict(torch.load(CACHE_DIR+'/image_classifier.pt'))\n",
        "    set_parameter_requires_grad(image_classifier, True)\n",
        "  except Exception as e:\n",
        "    print(\"Exception: \" + str(e))\n",
        "    print(\"Please make sure the 'image_classifier.pt' file exists\")\n",
        "\n",
        "image_classifier.fc = torch.nn.Identity()"
      ],
      "metadata": {
        "id": "inqs4FyTqi7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "  def __init__(self, num_classes, hidden_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.lstm = torch.nn.LSTM(hidden_size, hidden_size, 3, batch_first=True)\n",
        "    self.fc1 = torch.nn.Linear(hidden_size, 128)\n",
        "    self.fc2 = torch.nn.Linear(128, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    N = x.shape[0]  # current batch zise\n",
        "    x, (_, _) = self.lstm(x)\n",
        "    x = self.fc1(x[:,-1,:].view(N, -1))\n",
        "    x = torch.sigmoid(x)\n",
        "    x = torch.nn.functional.dropout(x, p=0.0)\n",
        "    x = self.fc2(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yXKJWLLxwvXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train video classifier"
      ],
      "metadata": {
        "id": "w8CvnHrd4_Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_to_use == 'CoAtNet':\n",
        "  hidden_size = 768\n",
        "else:\n",
        "  hidden_size = 2048\n",
        "\n",
        "video_classifier = LSTM(len(id_to_class.keys()), hidden_size).to(device)\n",
        "# video_classifier"
      ],
      "metadata": {
        "id": "2Li58foWQWYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(video_classifier)"
      ],
      "metadata": {
        "id": "tFbGStiLReBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 3\n",
        "# train_loader = DataLoader(\n",
        "#     UCF101(class_to_id, 'train', video_loader, _transform=tfs),\n",
        "#     collate_fn=custom_collate,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=True\n",
        "# )\n",
        "# dev_loader = DataLoader(\n",
        "#     UCF101(class_to_id, 'dev', video_loader, _transform=tfs),\n",
        "#     collate_fn=custom_collate,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=True\n",
        "# )\n",
        "# optimizer = Adam(video_classifier.parameters(), lr=0.003)\n",
        "# criterion = CrossEntropyLoss()\n",
        "\n",
        "# def handle_batch(model, criterion, batch, _device):\n",
        "#   captions, clips = batch\n",
        "#   # move tensors to device\n",
        "#   captions = captions.to(_device)\n",
        "#   clips = clips.to(_device)\n",
        "#   # encode with pre-trained image classifier\n",
        "#   N = clips.shape[0]  # current batch zise\n",
        "#   L = clips.shape[1]  # current sequence length\n",
        "#   clips = image_classifier(clips.view(-1, 3, IMG_SIZE, IMG_SIZE))\n",
        "#   H = clips.shape[-1] # current hidden size\n",
        "#   preds = model(clips.view(N, L, H))\n",
        "#   loss = criterion(preds, captions)\n",
        "#   return preds, loss\n",
        "\n",
        "# # try loading trained model\n",
        "# if os.path.exists(CACHE_DIR+'/video_classifier.pt'):\n",
        "#   video_classifier.load_state_dict(torch.load(CACHE_DIR+'/video_classifier.pt'))\n",
        "#   print(\"model state loaded\")\n",
        "\n",
        "# # training params\n",
        "# max_epoch = 5\n",
        "# early_stopping = -0.000001\n",
        "# show_train_loss = True   # disable this could speedup training\n",
        "\n",
        "\n",
        "# best_loss = None\n",
        "# for epoch in range(max_epoch):\n",
        "#   tot_train_loss = 0.0\n",
        "#   tot_eval_loss = 0.0\n",
        "#   video_classifier.train()\n",
        "#   with tqdm(total=len(train_loader)+len(dev_loader)) as pbar:\n",
        "#     for batch in train_loader:\n",
        "#       optimizer.zero_grad()\n",
        "#       _, loss = handle_batch(video_classifier, criterion, batch, device)\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       if show_train_loss:\n",
        "#         tot_train_loss += loss.item()\n",
        "#       pbar.update(1)\n",
        "#     video_classifier.eval()\n",
        "#     with torch.no_grad():\n",
        "#       for batch in dev_loader:\n",
        "#         _, loss = handle_batch(video_classifier, criterion, batch, device)\n",
        "#         tot_eval_loss += loss.item()\n",
        "#         pbar.update(1)\n",
        "  \n",
        "#   if show_train_loss:\n",
        "#     avg_train_loss = tot_train_loss / (len(train_loader) * batch_size)\n",
        "#     avg_eval_loss = tot_eval_loss / (len(dev_loader) * batch_size)\n",
        "#     tqdm.write(\"epoch={} avg_train_loss={:.4f} avg_eval_loss={:.4f}\".format(epoch, avg_train_loss, avg_eval_loss))\n",
        "#   else:\n",
        "#     avg_eval_loss = tot_eval_loss / (len(dev_loader) * batch_size)\n",
        "#     tqdm.write(\"epoch={} avg_eval_loss={:.4f}\".format(epoch, avg_eval_loss))\n",
        "\n",
        "#   # stopping criteria\n",
        "#   if best_loss is None:\n",
        "#     best_loss = tot_eval_loss\n",
        "#   if tot_eval_loss > (best_loss * (1+early_stopping)):\n",
        "#     tqdm.write(\"Eval loss not improving, stop training.\")\n",
        "#     break\n",
        "#   elif tot_eval_loss < best_loss:\n",
        "#     best_loss = tot_eval_loss\n",
        "#     # save weights periodically\n",
        "#     torch.save(video_classifier.state_dict(), CACHE_DIR+'/video_classifier.pt')"
      ],
      "metadata": {
        "id": "1Dtk1oNX48DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # try loading trained model\n",
        "# if os.path.exists(CACHE_DIR+'/video_classifier.pt'):\n",
        "#   video_classifier.load_state_dict(torch.load(CACHE_DIR+'/video_classifier.pt'))\n",
        "#   print(\"model state loaded\")\n",
        "\n",
        "# batch_size = 3\n",
        "# test_loader = DataLoader(\n",
        "#     UCF101(class_to_id, 'test', video_loader, _transform=tfs),\n",
        "#     collate_fn=custom_collate,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=False\n",
        "# )\n",
        "# criterion = CrossEntropyLoss()\n",
        "\n",
        "# def handle_batch(model, criterion, batch, _device):\n",
        "#   captions, clips = batch\n",
        "#   # move tensors to device\n",
        "#   captions = captions.to(_device)\n",
        "#   clips = clips.to(_device)\n",
        "#   # encode with pre-trained image classifier\n",
        "#   N = clips.shape[0]  # current batch zise\n",
        "#   L = clips.shape[1]  # current sequence length\n",
        "#   clips = image_classifier(clips.view(-1, 3, IMG_SIZE, IMG_SIZE))\n",
        "#   H = clips.shape[-1] # current hidden size\n",
        "#   preds = model(clips.view(N, L, H))\n",
        "#   loss = criterion(preds, captions)\n",
        "#   preds_ = torch.argmax(preds, dim=1)\n",
        "#   acc = (preds_ == captions)\n",
        "#   return acc, loss\n",
        "\n",
        "# video_classifier.eval()\n",
        "# tot_eval_loss = 0.0\n",
        "# with torch.no_grad():\n",
        "#   accs = []\n",
        "#   for batch in tqdm(test_loader):\n",
        "#     acc, loss = handle_batch(video_classifier, criterion, batch, device)\n",
        "#     tot_eval_loss += loss.cpu().item()\n",
        "#     accs.append(acc)\n",
        "#   accs = torch.cat(accs, dim=0)\n",
        "#   num_of_samples = accs.shape[0]\n",
        "#   avg_eval_loss = tot_eval_loss / (len(test_loader) * batch_size * FRAMES_PER_CLIP)\n",
        "#   tqdm.write(\"accuracy={:.4f}, avg loss={:.4f}\".format(accs.sum().item() / num_of_samples, avg_eval_loss))"
      ],
      "metadata": {
        "id": "ZHn4dUBQz6qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN+RNN"
      ],
      "metadata": {
        "id": "CVBPqjCDilK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResCNNEncoder(nn.Module):\n",
        "    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(ResCNNEncoder, self).__init__()\n",
        "\n",
        "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n",
        "        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n",
        "        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n",
        "        \n",
        "    def forward(self, x_3d):\n",
        "        cnn_embed_seq = []\n",
        "        for t in range(x_3d.size(1)):\n",
        "            # ResNet CNN\n",
        "            with torch.no_grad():\n",
        "                x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\n",
        "                x = x.view(x.size(0), -1)             # flatten output of conv\n",
        "\n",
        "            # FC layers\n",
        "            x = self.bn1(self.fc1(x))\n",
        "            x = F.relu(x)\n",
        "            x = self.bn2(self.fc2(x))\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "            x = self.fc3(x)\n",
        "\n",
        "            cnn_embed_seq.append(x)\n",
        "\n",
        "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
        "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
        "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
        "\n",
        "        return cnn_embed_seq"
      ],
      "metadata": {
        "id": "qpoziYuRiwEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.RNN_input_size = CNN_embed_dim\n",
        "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
        "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
        "        self.h_FC_dim = h_FC_dim\n",
        "        self.drop_p = drop_p\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.LSTM = nn.LSTM(\n",
        "            input_size=self.RNN_input_size,\n",
        "            hidden_size=self.h_RNN,        \n",
        "            num_layers=h_RNN_layers,       \n",
        "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
        "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, x_RNN):\n",
        "        \n",
        "        self.LSTM.flatten_parameters()\n",
        "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  \n",
        "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\" \n",
        "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
        "\n",
        "        # FC layers\n",
        "        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zUqBeJv6iyVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EncoderCNN architecture\n",
        "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
        "CNN_embed_dim = 512      # latent dim extracted by 2D CNN\n",
        "img_x, img_y = 256, 342  # resize video 2d frame size\n",
        "dropout_p = 0.0          # dropout probability\n",
        "\n",
        "# DecoderRNN architecture\n",
        "RNN_hidden_layers = 3\n",
        "RNN_hidden_nodes = 512\n",
        "RNN_FC_dim = 256\n",
        "\n",
        "image_classifier = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=300)\n",
        "video_classifier = DecoderRNN(CNN_embed_dim=300, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=len(id_to_class.keys()))"
      ],
      "metadata": {
        "id": "InHP054ajCYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(encoder_model, decoder_model, optimizer, criterion, train_loader, dev_loader, _device):\n",
        "  encoder_model.train()\n",
        "  decoder_model.train()\n",
        "\n",
        "  losses = []\n",
        "  scores = []\n",
        "\n",
        "  with tqdm(total=len(train_loader)+len(dev_loader)) as pbar:\n",
        "    for batch in train_loader:\n",
        "      captions, clips = batch\n",
        "      captions = captions.to(_device)\n",
        "      clips = clips.to(_device)\n",
        "\n",
        "      N = clips.shape[0]  # current batch size\n",
        "      L = clips.shape[1]  # current sequence length\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # encoded = encoder_model(clips.view(-1, clips.shape[2], clips.shape[3], clips.shape[4]))\n",
        "      # outputs = decoder_model(encoded.view(N, L, -1))\n",
        "      encoded = encoder_model(clips)\n",
        "      outputs = decoder_model(encoded)\n",
        "\n",
        "      loss = criterion(outputs, captions)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      pbar.update(1)\n",
        "    \n",
        "    encoder_model.eval()\n",
        "    decoder_model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in dev_loader:\n",
        "        captions, clips = batch\n",
        "        captions = captions.to(_device)\n",
        "        clips = clips.to(_device)\n",
        "\n",
        "        N = clips.shape[0]  # current batch size\n",
        "        L = clips.shape[1]  # current sequence length\n",
        "\n",
        "        # encoded = encoder_model(clips.view(-1, clips.shape[2], clips.shape[3], clips.shape[4]))\n",
        "        # outputs = decoder_model(encoded.view(N, L, -1))\n",
        "        encoded = encoder_model(clips)\n",
        "        outputs = decoder_model(encoded)\n",
        "\n",
        "        loss = criterion(outputs, captions)\n",
        "        losses.append(loss.item())\n",
        "        preds = torch.max(outputs, 1)[1]\n",
        "        score = accuracy_score(captions.cpu().data.squeeze().numpy(), preds.cpu().data.squeeze().numpy())\n",
        "        scores.append(score)\n",
        "        pbar.update(1)\n",
        "  \n",
        "  return sum(losses) / len(losses), sum(scores) / len(scores)"
      ],
      "metadata": {
        "id": "OvPf8w1ohDoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(encoder_model, decoder_model, criterion, test_loader, _device):\n",
        "  encoder_model.eval()\n",
        "  decoder_model.eval()\n",
        "\n",
        "  losses = []\n",
        "  scores = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "      captions, clips = batch\n",
        "      captions = captions.to(_device)\n",
        "      clips = clips.to(_device)\n",
        "\n",
        "      N = clips.shape[0]  # current batch size\n",
        "      L = clips.shape[1]  # current sequence length\n",
        "\n",
        "      # encoded = encoder_model(clips.view(-1, clips.shape[2], clips.shape[3], clips.shape[4]))\n",
        "      # outputs = decoder_model(encoded.view(N, L, -1))\n",
        "      encoded = encoder_model(clips)\n",
        "      outputs = decoder_model(encoded)\n",
        "\n",
        "      loss = criterion(outputs, captions)\n",
        "      losses.append(loss.item())\n",
        "      preds = torch.max(outputs, 1)[1]\n",
        "      score = accuracy_score(captions.cpu().data.squeeze().numpy(), preds.cpu().data.squeeze().numpy())\n",
        "      scores.append(score)\n",
        "  \n",
        "  return sum(losses) / len(losses), sum(scores) / len(scores)"
      ],
      "metadata": {
        "id": "O8VC1_Y-rPuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "max_epoch = 10\n",
        "early_stopping = 0.01\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    UCF101(class_to_id, 'train', video_loader, _transform=tfs),\n",
        "    collate_fn=custom_collate,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "dev_loader = DataLoader(\n",
        "    UCF101(class_to_id, 'dev', video_loader, _transform=tfs),\n",
        "    collate_fn=custom_collate,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    UCF101(class_to_id, 'test', video_loader, _transform=tfs),\n",
        "    collate_fn=custom_collate,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "train_params = list(image_classifier.parameters()) + list(video_classifier.parameters())\n",
        "optimizer = Adam(train_params, lr=learning_rate)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "best_loss = 99.9\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "  train_loss, train_acc = train(image_classifier, video_classifier, optimizer, criterion, train_loader, dev_loader, device)\n",
        "  if train_loss < best_loss:\n",
        "    torch.save(image_classifier.state_dict(), CACHE_DIR+'/image_classifier.pt')\n",
        "    torch.save(video_classifier.state_dict(), CACHE_DIR+'/video_classifier.pt')\n",
        "    best_loss = train_loss\n",
        "  if train_loss > best_loss * (1 + early_stopping):\n",
        "    print(\"Training loss not improving, stop training.\")\n",
        "    break\n",
        "  print(\"Epoch {}: train loss = {:.4f}, train acc = {:.4f}\".format(epoch, train_loss, train_acc))\n",
        "\n",
        "test_loss, test_acc = eval(image_classifier, video_classifier, criterion, test_loader, device)"
      ],
      "metadata": {
        "id": "DtJJGV8HsLYY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}